# main.py
import os
import argparse
# --- ìˆ˜ì •ëœ ë¶€ë¶„: ëª¨ë“  ëª¨ë“ˆì„ 'modules' í´ë”ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½ ---
from modules import config
from modules.crawler import RecipeCrawler
from modules.preprocess import DataPreprocessor
from modules.vector_store import VectorStoreManager
from modules.retriever import AdvancedRetriever
from modules.llm_handler import LLMHandler
# -------------------------------------------------------------
# Using built-in sqlite3 instead of pysqlite3 to avoid import issues
import sqlite3

# --- ì¶”ê°€/ìˆ˜ì •ëœ ë¶€ë¶„ ---
def main(rebuild_db: bool, until_step: str):
    """
    QA ì—”ì§„ì˜ ì „ì²´ ì‹¤í–‰ íë¦„ì„ ì œì–´í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    # 1. í¬ë¡¤ë§
    if not os.path.exists(config.CRAWLED_DATA_DIR) or not os.listdir(config.CRAWLED_DATA_DIR):
        print("--- 1. ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘ ---")
        os.makedirs(config.CRAWLED_DATA_DIR, exist_ok=True)
        crawler = RecipeCrawler()
        crawler.run(start_page=1, end_page=3, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_1-3.json"))
        crawler.run(start_page=11, end_page=20, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_11-20.json"))
        crawler.run(start_page=21, end_page=30, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_21-30.json"))
        crawler.run(start_page=31, end_page=40, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_31-40.json"))
        crawler.run(start_page=41, end_page=50, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_41-50.json"))
        crawler.run(start_page=51, end_page=60, output_filename=os.path.join(config.CRAWLED_DATA_DIR, "baek_recipes_51-60.json"))
    else:
        print(f"--- 1. í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸° ('{config.CRAWLED_DATA_DIR}' í´ë”ì— íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤) ---")

    # 'crawl' ë‹¨ê³„ê¹Œì§€ë§Œ ì‹¤í–‰í•˜ëŠ” ì˜µì…˜ í™•ì¸
    if until_step == 'crawl':
        print("\nSUCCESS: 'crawl' ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    preprocessing_needed = not os.path.exists(config.MERGED_PREPROCESSED_FILE)
    if preprocessing_needed:
        print("\n--- 2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ---")
        preprocessor = DataPreprocessor()
        success = preprocessor.run(config.CRAWLED_DATA_DIR, config.MERGED_PREPROCESSED_FILE)
        
        if not success:
            print("CRITICAL: ë°ì´í„° ì „ì²˜ë¦¬ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
    else:
        print(f"--- 2. ì „ì²˜ë¦¬ ê±´ë„ˆë›°ê¸° ('{config.MERGED_PREPROCESSED_FILE}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤) ---")

    # 'preprocess' ë‹¨ê³„ê¹Œì§€ë§Œ ì‹¤í–‰í•˜ëŠ” ì˜µì…˜ í™•ì¸
    if until_step == 'preprocess':
        print("\nSUCCESS: 'preprocess' ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # --- ì´í•˜ ì½”ë“œëŠ” until_step == 'run' ì¼ ë•Œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤. ---
    
    # 3. ë²¡í„° DB êµ¬ì¶• ë˜ëŠ” ë¡œë“œ
    print("\n--- 3. ë²¡í„° DB ì¤€ë¹„ ì‹œì‘ ---")
    vs_manager = VectorStoreManager()
    
    if rebuild_db or not os.path.exists(config.CHROMA_DB_PATH):
        if rebuild_db: print("INFO: --rebuild-db ì˜µì…˜ì— ë”°ë¼ DBë¥¼ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
        vectorstore = vs_manager.build(json_path=config.MERGED_PREPROCESSED_FILE)
    else:
        vectorstore = vs_manager.load()
    
    if not vectorstore:
        print("CRITICAL: ë²¡í„° DB ì¤€ë¹„ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 4. Advanced RAG ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
    print("\n--- 4. RAG ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ---")
    docs_for_retriever = vs_manager._load_documents_from_json(config.MERGED_PREPROCESSED_FILE)
    adv_retriever = AdvancedRetriever(vectorstore)
    retriever = adv_retriever.get_retriever()
    if docs_for_retriever:
        # ChromaDB ë°°ì¹˜ í¬ê¸° ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ë¬¸ì„œë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        # ParentDocumentRetrieverëŠ” ë‚´ë¶€ì ìœ¼ë¡œ child chunksë¥¼ ìƒì„±í•˜ë¯€ë¡œ ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
        batch_size = 100  # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì„¤ì •í•˜ì—¬ child chunks ìƒì„± ì‹œ í•œê³„ ì´ˆê³¼ ë°©ì§€
        total_docs = len(docs_for_retriever)
        print(f"INFO: ì´ {total_docs}ê°œ ë¬¸ì„œë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        for i in range(0, total_docs, batch_size):
            batch_end = min(i + batch_size, total_docs)
            batch_docs = docs_for_retriever[i:batch_end]
            print(f"INFO: ë°°ì¹˜ {i//batch_size + 1}: {len(batch_docs)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
            try:
                retriever.add_documents(batch_docs)
            except Exception as e:
                print(f"WARNING: ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
                smaller_batch_size = 10
                print(f"INFO: ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°({smaller_batch_size})ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                for j in range(i, batch_end, smaller_batch_size):
                    small_batch_end = min(j + smaller_batch_size, batch_end)
                    small_batch_docs = docs_for_retriever[j:small_batch_end]
                    try:
                        retriever.add_documents(small_batch_docs)
                        print(f"INFO: ì†Œë°°ì¹˜ {(j-i)//smaller_batch_size + 1} ì²˜ë¦¬ ì™„ë£Œ")
                    except Exception as small_e:
                        print(f"ERROR: ì†Œë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {small_e}")
                        continue
        
        print("INFO: ParentDocumentRetriever ì„¤ì • ì™„ë£Œ.")
    else:
        print("WARNING: ë¦¬íŠ¸ë¦¬ë²„ì— ì¶”ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 5. LLM í•¸ë“¤ëŸ¬ ë° RAG ì²´ì¸ ìƒì„±
    print("\n--- 5. QA ì—”ì§„(LLM) ì´ˆê¸°í™” ---")
    llm_handler = LLMHandler(retriever=retriever)
    qa_chain = llm_handler.create_rag_chain()
    print("SUCCESS: ë°±ì¢…ì› ë ˆì‹œí”¼ QA ì—”ì§„ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")

    # 6. ëŒ€í™”í˜• QA ì„¸ì…˜ ì‹œì‘
    print("\n--- ì•ˆë…•í•˜ì„¸ìš”! ë°±ì£¼ë¶€ì…ë‹ˆë‹¤. ë­ë“  ë¬¼ì–´ë³´ì…”ìœ  (ì¢…ë£Œí•˜ë ¤ë©´ 'ê·¸ë§Œ') ---")
    session_id = "user_session_01" 
    
    while True:
        try:
            user_input = input("ğŸ¤” ì§ˆë¬¸: ")
            if user_input.lower() == 'ê·¸ë§Œ':
                print("\në‹¤ìŒì— ë˜ ì°¾ì•„ì£¼ì…”ìœ ! ë§›ìˆê²Œ í•´ë“œì„¸ìœ ~")
                break
            
            response = qa_chain.invoke(
                {"input": user_input},
                config={"configurable": {"session_id": session_id}}
            )
            
            print("\në°±ì£¼ë¶€ ğŸ’¬:")
            print(response['answer'])
            print("-" * 50)
            
        except KeyboardInterrupt:
            print("\në‹¤ìŒì— ë˜ ì°¾ì•„ì£¼ì…”ìœ ! ë§›ìˆê²Œ í•´ë“œì„¸ìœ ~")
            break
        except Exception as e:
            print(f"\nERROR: ì£„ì†¡í•´ìœ , ì²˜ë¦¬ ì¤‘ì— ë¬¸ì œê°€ ìƒê²¼ì–´ìœ : {e}")

if __name__ == '__main__':
    # --- ì¶”ê°€/ìˆ˜ì •ëœ ë¶€ë¶„: ì‹¤í–‰ ì˜µì…˜ ì¶”ê°€ ---
    parser = argparse.ArgumentParser(description="ë°±ì¢…ì› ë ˆì‹œí”¼ QA ì—”ì§„")
    parser.add_argument(
        '--rebuild-db',
        action='store_true',
        help="ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        '--until-step',
        type=str,
        choices=['crawl', 'preprocess', 'run'],
        default='run',
        help="ì‹¤í–‰í•  ë§ˆì§€ë§‰ ë‹¨ê³„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤: crawl, preprocess, run (ì „ì²´ ì‹¤í–‰)"
    )
    args = parser.parse_args()
    
    main(rebuild_db=args.rebuild_db, until_step=args.until_step)